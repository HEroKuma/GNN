{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "\n",
    "gcn_msg = fn.copy_src(src='h', out='m')\n",
    "gcn_reduce = fn.sum(msg='m', out='h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "    \n",
    "    def forward(self, g, feature):\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = feature\n",
    "            g.update_all(gcn_msg, gcn_reduce)\n",
    "            h = g.ndata['h']\n",
    "            return self.linear(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layer1): GCNLayer(\n",
      "    (linear): Linear(in_features=1433, out_features=16, bias=True)\n",
      "  )\n",
      "  (layer2): GCNLayer(\n",
      "    (linear): Linear(in_features=16, out_features=7, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer1 = GCNLayer(1433, 16)\n",
    "        self.layer2 = GCNLayer(16, 7)\n",
    "        \n",
    "    def forward(self, g, features):\n",
    "        x = F.relu(self.layer1(g, features))\n",
    "        x = self.layer2(g, x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data import citation_graph as citegrh\n",
    "import networkx as nx\n",
    "def load_cora_data():\n",
    "    data = citegrh.load_cora()\n",
    "    features = torch.FloatTensor(data.features)\n",
    "    labels = torch.LongTensor(data.labels)\n",
    "    train_mask = torch.BoolTensor(data.train_mask)\n",
    "    test_mask = torch.BoolTensor(data.test_mask)\n",
    "    g = DGLGraph(data.graph)\n",
    "    return g, features, labels, train_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, g, features, labels, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(g, features)\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask]\n",
    "        _, indices = torch.max(logits, dim=1)\n",
    "        correct = torch.sum(indices == labels)\n",
    "        return correct.item() * 1.0 / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/herokuma/miniconda3/envs/pytorch/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/herokuma/miniconda3/envs/pytorch/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Loss 2.0187 | Test Acc 0.2180 | Time(s) nan\n",
      "Epoch 00001 | Loss 1.8209 | Test Acc 0.3280 | Time(s) nan\n",
      "Epoch 00002 | Loss 1.6206 | Test Acc 0.3740 | Time(s) nan\n",
      "Epoch 00003 | Loss 1.4591 | Test Acc 0.4870 | Time(s) 0.0355\n",
      "Epoch 00004 | Loss 1.3291 | Test Acc 0.5230 | Time(s) 0.0362\n",
      "Epoch 00005 | Loss 1.2170 | Test Acc 0.5710 | Time(s) 0.0372\n",
      "Epoch 00006 | Loss 1.1236 | Test Acc 0.6030 | Time(s) 0.0365\n",
      "Epoch 00007 | Loss 1.0442 | Test Acc 0.6220 | Time(s) 0.0368\n",
      "Epoch 00008 | Loss 0.9734 | Test Acc 0.6400 | Time(s) 0.0369\n",
      "Epoch 00009 | Loss 0.9095 | Test Acc 0.6450 | Time(s) 0.0372\n",
      "Epoch 00010 | Loss 0.8516 | Test Acc 0.6580 | Time(s) 0.0374\n",
      "Epoch 00011 | Loss 0.7986 | Test Acc 0.6660 | Time(s) 0.0372\n",
      "Epoch 00012 | Loss 0.7493 | Test Acc 0.6790 | Time(s) 0.0367\n",
      "Epoch 00013 | Loss 0.7036 | Test Acc 0.6910 | Time(s) 0.0364\n",
      "Epoch 00014 | Loss 0.6605 | Test Acc 0.7050 | Time(s) 0.0363\n",
      "Epoch 00015 | Loss 0.6192 | Test Acc 0.7170 | Time(s) 0.0363\n",
      "Epoch 00016 | Loss 0.5800 | Test Acc 0.7320 | Time(s) 0.0365\n",
      "Epoch 00017 | Loss 0.5438 | Test Acc 0.7420 | Time(s) 0.0367\n",
      "Epoch 00018 | Loss 0.5108 | Test Acc 0.7550 | Time(s) 0.0368\n",
      "Epoch 00019 | Loss 0.4805 | Test Acc 0.7580 | Time(s) 0.0369\n",
      "Epoch 00020 | Loss 0.4523 | Test Acc 0.7640 | Time(s) 0.0367\n",
      "Epoch 00021 | Loss 0.4257 | Test Acc 0.7690 | Time(s) 0.0368\n",
      "Epoch 00022 | Loss 0.4007 | Test Acc 0.7720 | Time(s) 0.0369\n",
      "Epoch 00023 | Loss 0.3774 | Test Acc 0.7730 | Time(s) 0.0369\n",
      "Epoch 00024 | Loss 0.3558 | Test Acc 0.7780 | Time(s) 0.0369\n",
      "Epoch 00025 | Loss 0.3361 | Test Acc 0.7760 | Time(s) 0.0370\n",
      "Epoch 00026 | Loss 0.3179 | Test Acc 0.7790 | Time(s) 0.0370\n",
      "Epoch 00027 | Loss 0.3007 | Test Acc 0.7830 | Time(s) 0.0369\n",
      "Epoch 00028 | Loss 0.2845 | Test Acc 0.7850 | Time(s) 0.0370\n",
      "Epoch 00029 | Loss 0.2692 | Test Acc 0.7900 | Time(s) 0.0369\n",
      "Epoch 00030 | Loss 0.2549 | Test Acc 0.7910 | Time(s) 0.0370\n",
      "Epoch 00031 | Loss 0.2415 | Test Acc 0.7920 | Time(s) 0.0370\n",
      "Epoch 00032 | Loss 0.2288 | Test Acc 0.7930 | Time(s) 0.0370\n",
      "Epoch 00033 | Loss 0.2169 | Test Acc 0.7940 | Time(s) 0.0370\n",
      "Epoch 00034 | Loss 0.2059 | Test Acc 0.7950 | Time(s) 0.0370\n",
      "Epoch 00035 | Loss 0.1958 | Test Acc 0.7950 | Time(s) 0.0372\n",
      "Epoch 00036 | Loss 0.1862 | Test Acc 0.7960 | Time(s) 0.0371\n",
      "Epoch 00037 | Loss 0.1771 | Test Acc 0.7960 | Time(s) 0.0373\n",
      "Epoch 00038 | Loss 0.1684 | Test Acc 0.7970 | Time(s) 0.0373\n",
      "Epoch 00039 | Loss 0.1604 | Test Acc 0.7960 | Time(s) 0.0374\n",
      "Epoch 00040 | Loss 0.1529 | Test Acc 0.7940 | Time(s) 0.0373\n",
      "Epoch 00041 | Loss 0.1458 | Test Acc 0.7950 | Time(s) 0.0373\n",
      "Epoch 00042 | Loss 0.1392 | Test Acc 0.7960 | Time(s) 0.0373\n",
      "Epoch 00043 | Loss 0.1330 | Test Acc 0.7930 | Time(s) 0.0374\n",
      "Epoch 00044 | Loss 0.1271 | Test Acc 0.7890 | Time(s) 0.0374\n",
      "Epoch 00045 | Loss 0.1215 | Test Acc 0.7890 | Time(s) 0.0374\n",
      "Epoch 00046 | Loss 0.1163 | Test Acc 0.7900 | Time(s) 0.0375\n",
      "Epoch 00047 | Loss 0.1113 | Test Acc 0.7900 | Time(s) 0.0376\n",
      "Epoch 00048 | Loss 0.1066 | Test Acc 0.7910 | Time(s) 0.0376\n",
      "Epoch 00049 | Loss 0.1022 | Test Acc 0.7920 | Time(s) 0.0376\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "g, features, labels, train_mask, test_mask = load_cora_data()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-2)\n",
    "dur = []\n",
    "for epoch in range(50):\n",
    "    if epoch >=3:\n",
    "        t0 = time.time()\n",
    "\n",
    "    net.train()\n",
    "    logits = net(g, features)\n",
    "    logp = F.log_softmax(logits, 1)\n",
    "    loss = F.nll_loss(logp[train_mask], labels[train_mask])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch >=3:\n",
    "        dur.append(time.time() - t0)\n",
    "\n",
    "    acc = evaluate(net, g, features, labels, test_mask)\n",
    "    print(\"Epoch {:05d} | Loss {:.4f} | Test Acc {:.4f} | Time(s) {:.4f}\".format(\n",
    "            epoch, loss.item(), acc, np.mean(dur)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DGLGraph(num_nodes=2708, num_edges=10556,\n",
      "         ndata_schemes={}\n",
      "         edata_schemes={}) tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([2, 5, 4,  ..., 1, 0, 2]) tensor([ True,  True,  True,  ..., False, False, False], dtype=torch.bool) tensor([False, False, False,  ..., False, False, False], dtype=torch.bool)\n"
     ]
    }
   ],
   "source": [
    "g, features, labels, train_mask, test_mask = load_cora_data()\n",
    "print(g, features, labels, train_mask, test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import numpy as np\n",
    "import scipy.sparse as spp\n",
    "\n",
    "# Create a star graph from a pair of arrays (using ``numpy.array`` works too).\n",
    "u = th.tensor([0, 3, 4, 5, 1])\n",
    "v = th.tensor([0, 2, 3, 4, 5])\n",
    "star1 = dgl.DGLGraph((u, v))\n",
    "\n",
    "# Create the same graph in one go! Essentially, if one of the arrays is a scalar,\n",
    "# the value is automatically broadcasted to match the length of the other array\n",
    "# -- a feature called *edge broadcasting*.\n",
    "start2 = dgl.DGLGraph((0, v))\n",
    "\n",
    "# Create the same graph from a scipy sparse matrix (using ``scipy.sparse.csr_matrix`` works too).\n",
    "adj = spp.coo_matrix((np.ones(len(u)), (u.numpy(), v.numpy())))\n",
    "star3 = dgl.DGLGraph(adj)\n",
    "\n",
    "# Create the same graph from a list of integer pairs.\n",
    "elist = [(0, 1), (0, 2), (0, 3), (0, 4), (0, 5)]\n",
    "star4 = dgl.DGLGraph(elist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "g = dgl.DGLGraph()\n",
    "g.add_nodes(10)\n",
    "# A couple edges one-by-one\n",
    "for i in range(1, 4):\n",
    "    g.add_edge(i, 0)\n",
    "# A few more with a paired list\n",
    "src = list(range(5, 8)); dst = [0]*3\n",
    "g.add_edges(src, dst)\n",
    "# finish with a pair of tensors\n",
    "src = th.tensor([8, 9]); dst = th.tensor([0, 0])\n",
    "g.add_edges(src, dst)\n",
    "\n",
    "# Edge broadcasting will do star graph in one go!\n",
    "g.clear(); g.add_nodes(10)\n",
    "src = th.tensor(list(range(1, 10)));\n",
    "g.add_edges(src, 0)\n",
    "\n",
    "# Visualize the graph.\n",
    "# nx.draw(g.to_networkx(), with_labels=True)\n",
    "nx.draw(star1.to_networkx(), with_labels=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
